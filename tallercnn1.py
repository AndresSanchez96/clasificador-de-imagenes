# -*- coding: utf-8 -*-
"""TallerCNN1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1li8YzUNeb-dv_140fA_6BJvCF6wpu5tn

## Importacion y extraccion de archivos
"""

import zipfile
import io
data = zipfile.ZipFile(io.BytesIO(uploaded['64x64_SIGNS.zip']),'r')
data.extractall()

"""## Librerias"""

import cv2
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import  layers,models  
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.mobilenet import preprocess_input
from keras.preprocessing import image
import matplotlib.image as mpimg

"""## Procesamiento de imagenes"""

train_signs = '/content/64x64_SIGNS/train_signs'
test_sings = '/content/64x64_SIGNS/test_signs'
val_sings = '/content/64x64_SIGNS/val_signs'
train_image = []
train_labels = []

epocas = 20
longitud , altura = 64,64
batch_size = 32
filtrosConv1 = 64
filtrosConv2 = 64 
tamaño_filtro1 = (3,3)
tamaño_filtro2 = (2,2)
tamaño_pool = (2,2)
clases = 6
lr = 0.0005

"""## ImageDataGenerator"""

entrenamiento_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range = 0.3,
    zoom_range = 0.3,
    horizontal_flip=True)

validacion_datagen = ImageDataGenerator(rescale=1./255)

imagen_entrenamiento = entrenamiento_datagen.flow_from_directory(
    train_signs,
    target_size=(altura,longitud),
    batch_size = batch_size,
    class_mode = 'categorical'
)

imagen_validacion = validacion_datagen.flow_from_directory(
    val_sings,
    target_size=(altura,longitud),
    batch_size = batch_size,
    class_mode = 'categorical'
)

print(imagen_entrenamiento.class_indices)

pasos_entrenamiento = imagen_entrenamiento.n//imagen_entrenamiento.batch_size
pasos_validacion = imagen_validacion.n//imagen_validacion.batch_size

"""## Modelo 1 de entrenamiento de datos """

model = models.Sequential()

model.add(layers.Conv2D(filtrosConv1,tamaño_filtro1,padding='same',
                        input_shape=[64,64,3]))
model.add(layers.MaxPool2D(pool_size=tamaño_pool))

model.add(layers.Conv2D(filtrosConv2,tamaño_filtro2, padding='same'))
model.add(layers.MaxPool2D(pool_size=tamaño_pool))

model.add(layers.Flatten())

model.add(layers.Dense(100,activation='relu'))
model.add(layers.Dense(50,activation='relu'))
model.add(layers.Dense(clases,activation='softmax'))

model.summary()

model.compile(optimizer=optimizers.Adam(lr=lr),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(imagen_entrenamiento,
                              steps_per_epoch=pasos_entrenamiento,
                              epochs=epocas,
                              validation_data=imagen_validacion,
                              validation_steps=pasos_validacion)

acc = model.evaluate(imagen_entrenamiento)

plt.plot(history.history['accuracy'],label='accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show

"""### Prediccion modelo 1

"""

test_datagen = ImageDataGenerator()

test_generator = test_datagen.flow_from_directory(
    directory = test_sings,
    target_size=(altura,longitud),
    color_mode='rgb',
    batch_size=1,
    class_mode = None,
    shuffle=False,
    seed=42
)

STEP_SIZE_TEST = test_generator.n//test_generator.batch_size
test_generator.reset()
pred = model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)

predicted_class_indices = np.argmax(pred,axis=1)

print(predicted_class_indices) #indices de las predicciones

np.argmax(pred[4])

plt.figure()
plt.imshow(np.squeeze(test_generator[4]).astype(np.uint8))
plt.colorbar()
plt.grid(False)
plt.show()

labels = (imagen_entrenamiento.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

filenames = test_generator.filenames
results = pd.DataFrame({'Filename':filenames,
                        'Predictions':predictions})
results.to_csv('results1.csv',index=False)

"""## Modelo 2 de entrenamiento de datos """

model2 = models.Sequential()

model2.add(layers.Conv2D(filtrosConv1,tamaño_filtro1,padding='same',
                        input_shape=[64,64,3]))
model2.add(layers.MaxPool2D(pool_size=tamaño_pool))

model2.add(layers.Conv2D(filtrosConv2,tamaño_filtro2, padding='same'))
model2.add(layers.MaxPool2D(pool_size=tamaño_pool))

model2.add(layers.Flatten())

model2.add(layers.Dense(64,activation='sigmoid'))
model2.add(layers.Dense(64,activation='sigmoid'))
model2.add(layers.Dense(clases,activation='softmax'))

model2.summary()

model2.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history2 = model2.fit_generator(imagen_entrenamiento,
                              steps_per_epoch=pasos_entrenamiento,
                              epochs=epocas,
                              validation_data=imagen_validacion,
                              validation_steps=pasos_validacion)

plt.plot(history2.history['accuracy'],label='accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show

"""### Prediccion modelo 2

"""

STEP_SIZE_TEST = test_generator.n//test_generator.batch_size
test_generator.reset()
pred2 = model2.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)

predicted_class_indices2 = np.argmax(pred2,axis=1)

print(predicted_class_indices2)
